<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>SourceCodeAnalysis</title>
    <link>/sca/docs/tal-tech_go-zero/core/</link>
    <description>Recent content on SourceCodeAnalysis</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language><atom:link href="/sca/docs/tal-tech_go-zero/core/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title></title>
      <link>/sca/docs/tal-tech_go-zero/core/01_bloom_filter/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/sca/docs/tal-tech_go-zero/core/01_bloom_filter/</guid>
      <description>布隆过滤器 #  包 core/bloom 提供了一个布隆过滤器的实现。
什么是 布隆过滤器？ #  布隆过滤器是 burton Bloom 在 1970年提出来的一个 用来处理 元素是否在集合中的 方法。对于海量数据，使用布隆过滤器在判定某个元素是否在其中时具有极高的空间效率和查询效率。它不会漏判，但是可能误判（不在里面的被认为在里面）。
布隆过滤器基本原理 #  布隆过滤器由一个超大的位数组A和M个hash函数组成，它支持两种操作：
 添加元素K：  将K依次交给M个函数运算，得到M个下标，将A中下标的值都设置为1   查询元素K：  将K依次交给M个函数运算，得到M个下标，如果A中这些下标的值都是1，则代表元素（可能）存在，不然就是（一定）不存在    不支持删除。
布隆过滤器的误差率 #  有专门的分析。
布隆过滤器的应用 #   判断某个邮箱地址在不在上亿个邮箱地址中 判断某个URL爬虫是否扒取过 判断某个数据在数据库是否存在 &amp;hellip;  go-zero布隆过滤器的实现 #  go-zero 中使用redis来存储位数组，自定义了hash函数。
hash 函数 #  // 根据 key 得到 M 个下标 func (f *BloomFilter) getLocations(data []byte) []uint { locations := make([]uint, maps) // maps = 14 	for i := uint(0); i &amp;lt; maps; i++ { // 每次将 i 追加到数据本身中，使用同一个hash函数得到不同的下标 	hashValue := hash.</description>
    </item>
    
    <item>
      <title></title>
      <link>/sca/docs/tal-tech_go-zero/core/02_break/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/sca/docs/tal-tech_go-zero/core/02_break/</guid>
      <description>熔断器 #  包 `core/break 提供了一个熔断器的接口和两套实现。
什么是 熔断器？ #  服务端经常会面临的一个问题是服务器负载过高，比如某个热点新闻导致流量短时间激增，这个时候如果不做相应的保护，服务端可能发生宕机导致直接不可用。处理这种情况一般会：
 服务降级：比如返回静态数据 服务熔断：对部分请求直接返回服务不可用，对其他请求继续提供服务  熔断器就是完成熔断功能的组件。它应该决定是否为当前请求提供正常服务，还是直接拒绝。
go-zero熔断器的实现 #  接口定义 #  type ( // 判断一个错误返回是否依旧是成功的 	Acceptable func(err error) bool Breaker interface { // Name returns the name of the netflixBreaker. 	Name() string // 判断请求是否被允许  // 如果允许，调用方需要在调用成功后调用 promise.Accept(),失败后调用 promise.Reject  // 如果不被允许（限流了），返回的是 ErrServiceUnavailable 	Allow() (Promise, error) // DoWithFallbackAcceptable  // - 在未被限流时执行req  // - 被限流时执行fallback（回退） 	// - acceptable 检查 req 是否调用成功，即便req返回的err不是nil 	DoWithFallbackAcceptable(req func() error, fallback func(err error) error, acceptable Acceptable) error // DoWithFallbackAcceptable 的简化版  // acceptable 为 err == nil 	DoWithFallback(req func() error, fallback func(err error) error) error // DoWithFallbackAcceptable 的简化版  // fallback 逻辑为：nil 不做回退 	DoWithAcceptable(req func() error, acceptable Acceptable) error // DoWithFallbackAcceptable 的简化版  // acceptable 为 err == nil  // fallback 逻辑为：nil 不做回退 	Do(req func() error) error } ) 可以看见，类库实现的Breaker里面本质上只有两个接口：</description>
    </item>
    
    <item>
      <title></title>
      <link>/sca/docs/tal-tech_go-zero/core/03_collection/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/sca/docs/tal-tech_go-zero/core/03_collection/</guid>
      <description>集合 #  包 core/collection 实现了各种典型集合类型
RollingWindow #  限流的方案在 熔断器 里面讲过，Rolling Window 就是一个经典的算法。
为了解决计算一段时间Interval的请求次数，滑动窗口算法的做法为：将计算周期切成N个Bucket，每个请求过来后，将请求放入对应的Bucket中，当要计算window有多少个请求时，把相关所有的bucket的数据累计起来即可。
我们先看看数据结构定义：
type RollingWindow struct { lock sync.RWMutex // 有多少个计数器 size int // 每个计算器的计数周期 interval time.Duration // 窗口，由多个Bucket组成 win *window // 窗口的偏移，范围为 [0, size) offset int // 最后一次更新的时间 lastTime time.Duration ignoreCurrent bool } type window struct { buckets []*Bucket // 一个window有多个Bucket size int } type Bucket struct { Sum float64 Count int64 } 我们再看看行为定义：
// 添加个数 func (rw *RollingWindow) Add(v float64) // 通过这个函数拿到当前的状态 // 会找到多个Bucket，需要总数的话直接累加即可 func (rw *RollingWindow) Reduce(fn func(b *Bucket)) 其实算法也不是那么的高深，一个滑动窗口由多个bucket组成：</description>
    </item>
    
    <item>
      <title></title>
      <link>/sca/docs/tal-tech_go-zero/core/04_executor/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/sca/docs/tal-tech_go-zero/core/04_executor/</guid>
      <description>执行器 #  包 core/executors 实现了各种类型的执行器。
   执行器类型 执行器使用场景     PeriodicalExecutor 每隔一段时间执行一次   BulkExecutor 凑够多个执行一次   ChunkExecutor 凑够多个字节执行一次   DelayExecutor 延迟一段时间后执行   LessExecutor 一段时间内最多执行一次    满足条件周期触发 #  PeriodicalExecutor 是任务在时间上的聚集，BulkExecutor 是时间+任务数， ChunkExecutor 是 时间+字节数。它们都是任务在一定条件下的触发。
类库在实现时，提出了一个任务容器的接口，来描述触发条件和任务的执行方式：
type TaskContainer interface { // 添加任务，返回是否需要执行任务了  // 对于 BulkExecutor，就是任务数量到了 	AddTask(task interface{}) bool // 执行任务 	Execute(tasks interface{}) // 移除所有任务，并返回它们 	RemoveAll() interface{} } 作为BulkExecutor和ChunkExecutor的基础，PeriodicalExecutor的接口如下：</description>
    </item>
    
    <item>
      <title></title>
      <link>/sca/docs/tal-tech_go-zero/core/05_fx/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/sca/docs/tal-tech_go-zero/core/05_fx/</guid>
      <description>执行器 #  包 core/fx 实现了流式运算的API。
Java里面有流式运算相关的API。Stream 就如同一个迭代器（Iterator），单向，不可往复，数据只能遍历一次，遍历过一次后即用尽了，就好比流水从面前流过，一去不复返。而和迭代器又不同的是，Stream 可以并行化操作，迭代器只能命令式地、串行化操作。顾名思义，当使用串行方式去遍历时，每个 item 读完后再读下一个 item。而使用并行去遍历时，数据会被分成多个段，其中每一个都在不同的线程中处理，然后将结果一起输出。
Stream的数据结构 #  Stream只有一个数据成员：
type Stream struct { // 所有的数据都在chan里面 	source &amp;lt;-chan interface{} } Stream的工厂方法 #  创建流对象本质是是把多个同类型的元素放入chan中，类库提供了如下的工厂方法：
type GenerateFunc func(source chan&amp;lt;- interface{}) func From(generate GenerateFunc) Stream func Just(items ...interface{}) Stream func Range(source &amp;lt;-chan interface{}) Stream Stream的API #  对于Stream，有如下API：
 元素处理API  // 将n个数据放入新的Stream中 func (p Stream) Buffer(n int) Stream // 取 Top N func (p Stream) Head(n int64) Stream // 取 Head N func (p Stream) Tail(n int64) Stream // 排序元素 func (p Stream) Sort(less LessFunc) Stream // 反转元素 func (p Stream) Reverse() Stream // 将元素分组，Stream的chan里面每个元素为一组原始的元素 func (p Stream) Group(fn KeyFunc) Stream // 将元素分组，Stream的chan里面只有一个元素，为原始的所有的元素 func (p Stream) Merge() Stream // 过滤元素 func (p Stream) Filter(fn FilterFunc, opts .</description>
    </item>
    
    <item>
      <title></title>
      <link>/sca/docs/tal-tech_go-zero/core/06_hash/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/sca/docs/tal-tech_go-zero/core/06_hash/</guid>
      <description>一致性hash #  状态：编辑中
包 core/hash 实现了一个一致性hash。
hash 和 一致性hash #  hash函数将一个字符串（字节数组）转换为一个数值，比如md5。
在业务开发中，经常会使用hash函数将数据打散到多个存储实例上，比如：
 数据库水平分表：hash(用户的名字) 后和总表数取模确定存储在哪张表中 缓存集群挑选：hask(key) 后和 缓存集群格式取模确定数据缓存在哪个缓存集群中  realAddr = hash(key) % node_count 这种方式，在instance_count 不变的情况下是工作良好的，但是真实的生产环境却是动态的，就会有如下问题：
 容错性：实例宕机了，某个实例下线后，最坏情况（第一个实例下线）会导致所有的数据都失效，最好情况（最后一个实例下线）是 1/n 的数据失效 扩展性：添加新实例同上  一致性hash就是用来解决这个问题的。
普通的算法是将N个物理节点组成一个环，通过和N取模确定到底使用哪个具体的节点，每个节点对应一个物理节点
一致性hash先得到一个有N个虚拟节点的环，我们先认为N = 2^32 - 1。
 将每个机器节点hash后与N取模能够得到机器在虚拟环的位置P 通过Key和N取模确定是哪个虚拟节点n 顺着虚拟环顺时针查找到的第一个P就是需要使用的物理节点  我们来分析一下一致性hash的表现：
 容错性：实例宕机了，某个实例下线后，总是 1/n 的数据失效 扩展性：添加新实例，总是少于 1/n的数据受影响  有一个地方需要特别注意一下： 将每个机器节点hash后与N取模能够得到机器在虚拟环的位置P 这步，如果做得不够均匀，就会导致 数据倾斜，也就是每个物理节点的负载不一样。
比如有N物理节点，它们的P 相减为 2^32 / N 才比较对，极端情况P相差1，那么就有一台物理节点几乎承担所有的负载。
解决这个问题的一种方法为对每个物理节点进行多个hash，每次计算的结果为一个虚拟节点。同一个物理节点的多个虚拟节点都是该物理节点服务的范围。通过多个hash，降低数据倾斜的概率。
一致性hash的API #  type ( HashFunc func(data []byte) uint64 ConsistentHash struct { // 将key转换为int的函数 	hashFunc HashFunc // 副本的数量，也就是上面虚拟节点的数量 	replicas int // 	keys []uint64 // 	ring map[uint64][]interface{} // 用来确认某个节点是否存在的冗余数据 	nodes map[string]lang.</description>
    </item>
    
    <item>
      <title></title>
      <link>/sca/docs/tal-tech_go-zero/core/09_other/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/sca/docs/tal-tech_go-zero/core/09_other/</guid>
      <description>其他 #     包 说明     cmdline 命令行接收相关的工具函数   codec 加解密/编码相关的二次封装   conf yaml/json加解密, kv 的序列化和反序列化   contextx context 的二次封装   errorx error相关的二次封装，包括多个error和原子化操作的error   filex 文件相关的封装，比如读取文件第一行/最后一行    </description>
    </item>
    
  </channel>
</rss>
